[{"content":"Let´s take advantage of things that are already built. Im referring to sklearn and the whole variety of beautiful utilities and functions that help us make our lives simpler and stay curious, testing and trying out new stuff. This will be simple and straight to the bone.\nA. Splits First, lets take the whole data and leave aside a testing chunk. Then, we can run cross validation on training set and when we think we are ready, we can check our scores on the test set. Don´t fool yourself fitting out over the test set! Its there only for a final check and having a firm-ground metric on how our model can generalize out there in the wild.\nSecond, let´s define our kfold so we can have reproducible results from now on. Two things:\n  Choose your n_folds wisely: for little data, we need similar distributions over the target, 3-folds may be enough. Otherwise, 5-fold or even a 10-fold may be necessary.\n  Take a minute and think about which type of folding strategy to choose. Group folds, stratify multiple labels?\n  So, our first piece of code should look similar to the following.\nfrom sklearn.model_selection import KFold, train_test_split  X = df.drop(columns=[\u0026#39;y\u0026#39;]) y = df.y.values  # Define a test set folds = 5 X_train, X_test, y_train, y_test = train_test_split(  X,  y,  test_size=(1/folds),  random_state=0,  stratify=y)  # Make folds kfold = KFold(  n_splits=folds - 1 ,  shuffle=True,  random_state=2) B. Data cleaning This step should take care of cleaning the data so that it is ready to use. For example: strip, filter, transform, divide, concatenate columns, strings, data types, etc. One or multiple pandas functions should be enough. When done, generate a csv to keep track of your experiment.\nC. Data preprocessing Here starts the fun part. For preprocessing, we should focus on partitioning columns into numerical and categorical and transforming them. Then, concatenate both and gather all the data, so we can pass it on to a model.\nThe trick here is to use custom transformers so we have the flexibility of doing whatever we want. Two cases arise:\n from sklearn.base import BaseEstimator, TransformerMixin   # Case #1: no fit method required class DropFeatureSelector(BaseEstimator, TransformerMixin):  def __init__(self, variables):  self.variables = variables   def fit(self, X, y = None):  return self   def transform(self, X):  X_dropped = X.drop(self.variables, axis = 1)  self.columns = X_dropped.columns  return X_dropped   # Case #2: fit method required  class OneHotEncoderCustom(BaseEstimator, TransformerMixin):  def __init__(self, variables):  self.variables = variables  self.ohe = OneHotEncoder(drop=\u0026#39;first\u0026#39;, handle_unknown=\u0026#39;ignore\u0026#39;)   def fit(self, X, y = None):  X_ = X.loc[:,self.variables]  self.ohe.fit(X_)  return self   def transform(self, X):  X_ = X.loc[:,self.variables]   # get one-hot encoded feature in df format  X_transformed = pd.DataFrame(self.ohe.transform(X_).toarray(), columns= self.ohe.get_feature_names_out())   # Remove columns that are one hot encoded in original df  X.drop(self.variables, axis= 1, inplace=True)   # Add one hot encoded feature to original df  X[self.ohe.get_feature_names_out()] = X_transformed[self.ohe.get_feature_names_out()].values  return X Understanding custom transformers, now we should be able to build a pipeline in the following fashion.\nfrom sklearn.pipeline import Pipeline from sklearn.preprocessing import OrdinalEncoder from sklearn.impute import SimpleImputer from sklearn.compose import ColumnTransformer from sklearn.ensemble import HistGradientBoostingRegressor   # Categorical transformers ordinal_encoder = OrdinalEncoder(categories=categories)  categorical_preprocessor = Pipeline(  steps=[  #(\u0026#39;replacer\u0026#39;, custom_replacer(variables=[\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;])),  (\u0026#39;encoder\u0026#39;, ordinal_encoder)  ] )   # Numerical transformers imputer = SimpleImputer(strategy=\u0026#39;mean\u0026#39;)  numerical_preprocessor = Pipeline(  steps=[  (\u0026#39;imputer\u0026#39;, imputer)  ] )  # Complete preprocessor preprocessor = ColumnTransformer(  transformers=[  (\u0026#39;categorical\u0026#39;, categorical_preprocessor, categorical_columns),  (\u0026#39;numerical\u0026#39;, numerical_preprocessor, numerical_columns)  ] )  # Add estimator gbrt_pipeline = Pipeline([  (\u0026#39;preprocessor\u0026#39;, preprocessor),  (\u0026#39;model\u0026#39;, HistGradientBoostingRegressor(categorical_features=range(4))) ]) We can also check graphically our pipeline.\nfrom sklearn import set_config  set_config(display=\u0026#34;diagram\u0026#34;) gbrt_pipeline D. Evaluate our model Now, for a certain set of pre-defined hyperparameters, we can evaluate our pipeline.\ndef evaluate (model, X, y, cv):  cv_results = cross_validate(  model,  X,  y,  cv=cv,  scoring={\u0026#39;neg_mean_absolute_error\u0026#39;,\u0026#39;neg_root_mean_squared_error\u0026#39;},  )  rmse = -cv_results[\u0026#39;test_neg_root_mean_squared_error\u0026#39;]  mae = -cv_results[\u0026#39;test_neg_mean_absolute_error\u0026#39;]   print(  f\u0026#34;Mean Absolute Error: {mae.mean():.3f}+/- {mae.std():.3f}\\n\u0026#34;  f\u0026#34;Root Mean Squared Error: {rmse.mean():.3f}+/- {rmse.std():.3f}\u0026#34;  )   evaluate(gbrt_pipeline, X, y, cv=ts_cv) E. Grid Search Now, we can test any hyperparameter from the preprocessign pipeline and estimator.\nfrom sklearn.model_selection import GridSearchCV  def grid_search(model, X, y, params, cv):   grid_search = GridSearchCV(  gbrt_pipeline,  param_grid=params,  cv=ts_cv,  scoring={\u0026#39;neg_mean_absolute_error\u0026#39;,\u0026#39;neg_root_mean_squared_error\u0026#39;},  refit=\u0026#39;neg_root_mean_squared_error\u0026#39;,  n_jobs=-1)   grid_search.fit(X, y)   print(-grid_search.best_score_)  print(grid_search.best_params_)   # Run grid search params = {  \u0026#34;preprocessor__numerical__imputer__strategy\u0026#34;: [\u0026#39;mean\u0026#39;,\u0026#39;median\u0026#39;],  \u0026#34;model__learning_rate\u0026#34;: [0.01, 0.1], }  grid_search(my_pipeline, X_train, y_train, params, ts_cv) F. Special note on metrics! (Taken from calmcode.io) Often, it is important to create custom metrics that respond to business questions.\nfrom sklearn.model_selection import GridSearchCV from sklearn.metrics import precision_score, recall_score, make_scorer  def min_recall_precision(est, X, y_true, sample_weight=None):  y_pred = est.predict(X)  recall = recall_score(y_true, y_pred)  precision = precision_score(y_true, y_pred)  return min(recall, precision)  grid = GridSearchCV(  estimator=LogisticRegression(max_iter=1000),  param_grid={\u0026#39;class_weight\u0026#39;: [{0: 1, 1: v} for v in np.linspace(1, 20, 30)]},  scoring={\u0026#39;precision\u0026#39;: make_scorer(precision_score),  \u0026#39;recall\u0026#39;: make_scorer(recall_score),  \u0026#39;min_both\u0026#39;: min_recall_precision}, # custom metric  refit=\u0026#39;min_both\u0026#39;,  return_train_score=True,  cv=10,  n_jobs=-1 ) grid.fit(X, y, sample_weight=np.log(1 + df[\u0026#39;Amount\u0026#39;] )) ","permalink":"http://rafasacaan.github.io/pots-and-pans/posts/a-simple-workflow/","summary":"Let´s take advantage of things that are already built. Im referring to sklearn and the whole variety of beautiful utilities and functions that help us make our lives simpler and stay curious, testing and trying out new stuff. This will be simple and straight to the bone.\nA. Splits First, lets take the whole data and leave aside a testing chunk. Then, we can run cross validation on training set and when we think we are ready, we can check our scores on the test set.","title":"workflows with sklearn"},{"content":"Whenever starting a new project, it is good to get started as nice and easy as possible. The fundamental bricks for this task is to generate an environment and a basic tree of files and directories. And of course, the necessary tools to keep things working from then on.\nWhat else? We can treat our projects as packages right away, and get an awesome package manager such as poetry.\nConda, the environment-maker-manager. Let´s create the most basic python environment, where the most important thing is to have a defined working python version. To do this, we can create a conda lite-weight environment from the following .yaml file:\nname: my_env_name  channels:  - default  - conda-forge  dependencies:  - python=3.8 Please notice that a name and python version are defined inside the file. Now, we can create our very basic env.\nconda env create -f environment.yaml That´s it with conda!\nPoetry, the hard-working-friend. First, we need to install poetry using:\ncurl -sSL https://raw.githubusercontent.com/python-poetry/poetry/master/get-poetry.py | python - After installing poetry, we can run the following command on the terminal.\npoetry new my_project Several files will be created to get started, among these, a pyproject.toml file which is in charge to manage our dependencies.\nNow, we can do the following:\n We can install our dependencies, if added using poetry install. Install packages for production with poetry add \u0026lt;a-name\u0026gt; Install packages for development with poetry add \u0026lt;a-name\u0026gt; --dev Remove packages with poetry remove \u0026lt;a-name\u0026gt; Show packages with poetry show  Also, if we need to run an existing project, we can do poetry init to create the .toml file.\nSharing is caring Now, we can export our conda env, which was managed by poetry.\nconda env export \u0026gt; environment.yml With this file, we can recreate our environment with conda and run poetry install to get things up and running.\nEasy as a-b-c.\nNote! Something that helped a lot, was to restrict the space of python versions compatibility. I did this in the pyproject.toml file:\npython = \u0026#34;\u0026gt;=3.8,\u0026lt;3.9\u0026#34; ","permalink":"http://rafasacaan.github.io/pots-and-pans/posts/conda-poetry/","summary":"Whenever starting a new project, it is good to get started as nice and easy as possible. The fundamental bricks for this task is to generate an environment and a basic tree of files and directories. And of course, the necessary tools to keep things working from then on.\nWhat else? We can treat our projects as packages right away, and get an awesome package manager such as poetry.\nConda, the environment-maker-manager.","title":"conda + poetry"},{"content":"Remember, remember\u0026hellip;\u0026hellip;\na = c * 7  def hola():  \u0026#34;\u0026#34;\u0026#34; ahjahsjas \u0026#34;\u0026#34;\u0026#34;  a = 3 * tu  return hey dsada dasd asd adas fdaf sfadadasdasd.\n","permalink":"http://rafasacaan.github.io/pots-and-pans/posts/first-blog/","summary":"Remember, remember\u0026hellip;\u0026hellip;\na = c * 7  def hola():  \u0026#34;\u0026#34;\u0026#34; ahjahsjas \u0026#34;\u0026#34;\u0026#34;  a = 3 * tu  return hey dsada dasd asd adas fdaf sfadadasdasd.","title":"trees and rainbows"},{"content":"This is a cheat list for using virtualenv to create python environments, and pip as an install manager.\nvirtualenv. Create a virtualenv:\n python3 -m virtualenv my_env\n Activate the env:\n source my_env/bin/activate\n Deactivate:\n deactivate\n Remove an env:\n rm -rf my_env\n pip Generate file with packages:\n my_env/bon/python -m pip freeze \u0026gt; requirements.txt\n Install packages in env from file:\n my_env/bon/python -m pip install -r requirements.txt\n List packages:\n pip list\n Install new packages:\n python3 -m pip install jupyterlab scikit-learn pandas numpy spicy matplotlib\n End! ","permalink":"http://rafasacaan.github.io/pots-and-pans/posts/virtualenv-pip/","summary":"This is a cheat list for using virtualenv to create python environments, and pip as an install manager.\nvirtualenv. Create a virtualenv:\n python3 -m virtualenv my_env\n Activate the env:\n source my_env/bin/activate\n Deactivate:\n deactivate\n Remove an env:\n rm -rf my_env\n pip Generate file with packages:\n my_env/bon/python -m pip freeze \u0026gt; requirements.txt\n Install packages in env from file:\n my_env/bon/python -m pip install -r requirements.","title":"virutalenv + pip"}]